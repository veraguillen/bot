#!/usr/bin/env python3
"""
Script para probar especÃ­ficamente el LLM
"""
import asyncio
import json
from app.api.llm_client import generate_chat_completion, _get_validated_base_url
from app.core.config import settings

async def test_llm():
    """Prueba el LLM directamente"""
    
    print("ğŸ” Probando LLM directamente...")
    
    try:
        # Verificar configuraciÃ³n
        base_url = _get_validated_base_url()
        print(f"ğŸ”§ URL base: {base_url}")
        print(f"ğŸ”§ Modelo: {settings.OPENROUTER_MODEL_CHAT}")
        print(f"ğŸ”§ API Key configurada: {bool(settings.OPENROUTER_API_KEY)}")
        
        # Probar con un mensaje simple
        system_message = "Eres un asistente Ãºtil y profesional."
        user_message = "Hola, Â¿cÃ³mo estÃ¡s?"
        
        print(f"System: {system_message}")
        print(f"User: {user_message}")
        
        # Verificar que el modelo se estÃ¡ leyendo correctamente
        print(f"ğŸ”§ Modelo que se enviarÃ¡: {settings.OPENROUTER_MODEL_CHAT}")
        
        response = await generate_chat_completion(
            system_message=system_message,
            user_message=user_message
        )
        
        print(f"âœ… Respuesta LLM: {response}")
        return True
        
    except Exception as e:
        print(f"âŒ Error en LLM: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """FunciÃ³n principal"""
    print("ğŸš€ Probando LLM...")
    print("=" * 50)
    
    success = await test_llm()
    
    if success:
        print("âœ… LLM funciona correctamente")
    else:
        print("âŒ LLM tiene problemas")

if __name__ == "__main__":
    asyncio.run(main()) 